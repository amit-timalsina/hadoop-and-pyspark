# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tFoaJ8KhfuiCHaUnXtOoDBMQYTtQ0t4W
"""

import pyspark 
from pyspark.sql import SparkSession
from pyspark.sql.types import *



spark = SparkSession.builder\
	.master("local").appName("hdfs_test").getOrCreate()

# booksSchema = StructType() \
#                     	.add("id", "integer")\
#                     	.add("book_title", "string")\
#                     	.add("publish_or_not", "string")\
#                     	.add("technology", "string")

df =spark.read.csv("hdfs://localhost:9000/user/hdoop/data/cars.csv", header=True,  inferSchema=True)
df.show(5)


df = spark.read.csv("hdfs://localhost:9000/user/hdoop/data/cars.csv", header=True, sep = ';')
df.show(5)


print(df.limit(5))
#print()

print(df.dtypes)

print(df.printSchema())       # print the schema. it is all string by default

df = spark.read.csv("hdfs://localhost:9000/user/hdoop/data/cars.csv", header=True, sep = ';', inferSchema=True)
print(df.printSchema())

#Defining Schema Explicitly
print(df.columns)

labels = [
     ('Car',StringType()),
     ('MPG',DoubleType()),
     ('Cylinders',IntegerType()),
     ('Displacement',DoubleType()),
     ('Horsepower',DoubleType()),
     ('Weight',DoubleType()),
     ('Acceleration',DoubleType()),
     ('Model',IntegerType()),
     ('Origin',StringType())
]

schema = StructType([StructField (x[0], x[1], True) for x in labels])

df = spark.read.csv("hdfs://localhost:9000/user/hdoop/data/cars.csv", header=True, sep = ';', inferSchema=True)
print(df.printSchema())

print(df.show(truncate=False))

#As we can see here, the data has been successully loaded with the specified datatypes.

#DataFrame Operations on Columns

# 1st method
# Column name is case sensitive in this usage
print(df.Car)
print("*"*20)
print(df.select(df.Car).show(truncate=False))

# 2nd method
# Column name is case insensitive here
print(df['car'])
print("*"*20)
print(df.select(df['car']).show(truncate=False))

#Selecting Multiple Columns

print(df.Car, df.Cylinders)
print("*"*40)
print(df.select(df.Car, df.Cylinders).show(truncate=False))

#Adding New Columns 
# CASE 1: Adding a new column
# We will add a new column called 'first_column' at the end
from pyspark.sql.functions import lit
df = df.withColumn('first_column',lit(1)) 
# lit means literal. It populates the row with the literal value given.
# When adding static data / constant values, it is a good practice to use it.
print(df.show(5,truncate=False))

# CASE 2: Adding multiple columns
# We will add two new columns called 'second_column' and 'third_column' at the end
df = df.withColumn('second_column', lit(2)) \
       .withColumn('third_column', lit('Third Column')) 

print(df.show(5,truncate=False))


# CASE 3: Deriving a new column from an exisitng one
# We will add a new column called 'car_model' which has the value of car and model appended together with a space in between 

from pyspark.sql.functions import concat
df = df.withColumn('car_model', concat(col("Car"), lit(" "), col("model")))



print(df.show(5,truncate=False))

#Renaming Columns
df = df.withColumnRenamed('first_column', 'new_column_one') \
       .withColumnRenamed('second_column', 'new_column_two') \
       .withColumnRenamed('third_column', 'new_column_three')
print(df.show(truncate=False))

#Grouping By Columns

# Group By a column in PySpark
print(df.groupBy('Origin').count().show(5))

#Removing Columns
df = df.drop('new_column_one')
print(df.show(5,truncate=False))

#Remove multiple columnss in one go
df = df.drop('new_column_two') \
       .drop('new_column_three')
print(df.show(5,truncate=False))

#DataFrame Operations on Rows

# Filtering Rows
total_count = df.count()
print("TOTAL RECORD COUNT: " + str(total_count)) 
europe_filtered_count = df.filter(col('Origin')=='Europe').count()
print("EUROPE FILTERED RECORD COUNT: " + str(europe_filtered_count))
print(df.filter(col('Origin')=='Europe').show(truncate=False))

# Filtering rows in PySpark based on Multiple conditions
total_count = df.count()
print("TOTAL RECORD COUNT: " + str(total_count)) 
europe_filtered_count = df.filter((col('Origin')=='Europe') & 
                                  (col('Cylinders')==4)).count() # Two conditions added here
print("EUROPE FILTERED RECORD COUNT: " + str(europe_filtered_count))
print(df.filter(col('Origin')=='Europe').show(truncate=False))

#Get Unique Rows in PySpark
print(df.select('Origin').distinct().show())

#Get Unique Rows in PySpark based on mutliple columns
print(df.select('Origin','model').distinct().show())

# Sort Rows in PySpark
# By default the data will be sorted in ascending order
print(df.orderBy('Cylinders').show(truncate=False))

# To change the sorting order, you can use the ascending parameter
print(df.orderBy('Cylinders', ascending=False).show(truncate=False))

# Using groupBy aand orderBy together
print(df.groupBy("Origin").count().orderBy('count', ascending=False).show(10))

# Union Dataframes
# CASE 1: Union When columns are in order

df = spark.read.csv("hdfs://localhost:9000/user/hdoop/data/cars.csv", header=True, sep = ';', inferSchema=True)
europe_cars = df.filter((col('Origin')=='Europe') & (col('Cylinders')==5))
japan_cars = df.filter((col('Origin')=='Japan') & (col('Cylinders')==3))
print("EUROPE CARS: "+str(europe_cars.count()))
print("JAPAN CARS: "+str(japan_cars.count()))
print("AFTER UNION: "+str(europe_cars.union(japan_cars).count()))

# CASE 1: Union When columns are not in order
# Creating two dataframes with jumbled columns
df1 = spark.createDataFrame([[1, 2, 3]], ["col0", "col1", "col2"])
df2 = spark.createDataFrame([[4, 5, 6]], ["col1", "col2", "col0"])
print(df1.unionByName(df2).show())

# Common Data Manipulation Functions
# Functions available in PySpark

from pyspark.sql import functions
# Similar to python, we can use the dir function to view the avaiable functions
print(dir(functions))


# Loading the data
from pyspark.sql.functions import col, min, max, lit, concat,lower, upper, substring, to_date, to_timestamp, date_add, date_sub
df = spark.read.csv("hdfs://localhost:9000/user/hdoop/data/cars.csv", header=True, sep = ';', inferSchema=True)

# Prints out the details of a function
print(help(substring))

# alias is used to rename the column name in the output
print(df.select(col('Car'),lower(col('Car')),upper(col('Car')),substring(col('Car'),1,4).alias("concatenated value")).show(5, False))

print(df.select(col("Car"),col("model"),concat(col("Car"), lit(" "), col("model"))).show(5, False))

# Numeric functions
print(df.select(min(col('Weight')), max(col('Weight'))).show())


print(df.select(min(col('Weight'))+lit(10), max(col('Weight')+lit(10))).show())

# Operations on Date
df = spark.createDataFrame([('2019-12-25 13:30:00',)], ['DOB'])
print(df.show())
print(df.printSchema())

df = spark.createDataFrame([('2019-12-25 13:30:00',)], ['DOB'])
print(df.show())
print(df.printSchema())

df = spark.createDataFrame([('25/Dec/2019 13:30:00',)], ['DOB'])
df = df.select(to_date(col('DOB'),'dd/MMM/yyyy HH:mm:ss'), to_timestamp(col('DOB'),'dd/MMM/yyyy HH:mm:ss'))
print(df.show())
print(df.printSchema())


# create a dummy dataframe
df = spark.createDataFrame([('1990-01-01',),('1995-01-03',),('2021-03-30',)], ['Date'])
# find out the required dates
print(df.select(date_add(max(col('Date')),3), date_sub(min(col('Date')),3)).show())

# Create two dataframes
cars_df = spark.createDataFrame([[1, 'Car A'],[2, 'Car B'],[3, 'Car C']], ["id", "car_name"])
car_price_df = spark.createDataFrame([[1, 1000],[2, 2000],[3, 3000]], ["id", "car_price"])
print(cars_df.show())
print(car_price_df.show())

# Executing an inner join so we can see the id, name and price of each car in one row
print(cars_df.join(car_price_df, cars_df.id == car_price_df.id, 'inner').select(cars_df['id'],cars_df['car_name'],car_price_df['car_price']).show(truncate=False))

# Spark SQL


df = spark.read.csv("hdfs://localhost:9000/user/hdoop/data/cars.csv", header=True, sep = ';', inferSchema=True)
# Register Temporary Table
df.createOrReplaceTempView("temp")
# Select all data from temp table
spark.sql("select * from temp limit 5").show()
# Select count of data in table
spark.sql("select count(*) as total_count from temp").show()

# As you can see, we registered the dataframe as temporary table and then ran basic SQL queries on it.

'''RDD
With map, you define a function and then apply it record by record. 
Flatmap returns a new RDD by first applying a function to all of the elements in RDDs and then flattening the result. 
Filter, returns a new RDD. Meaning only the elements that satisfy a condition. With reduce, we are taking neighboring elements and producing a single combined result. For example, let's say you have a set of numbers. 
You can reduce this to its sum by providing a function that takes as input two values and reduces them to one.'''

cars = spark.sparkContext.textFile('hdfs://localhost:9000/user/hdoop/data/cars.csv')
print(cars.first())
cars_header = cars.first()
cars_rest = cars.filter(lambda line: line!=cars_header)
print(cars_rest.first())

# How many cars are there in our csv data?

print(cars_rest.map(lambda line: line.split(";")).count())

# Display the Car name, MPG, Cylinders, Weight and Origin for the cars Originating in Europe

# Car name is column  0
print((cars_rest.filter(lambda line: line.split(";")[8]=='Europe').
 map(lambda line: (line.split(";")[0],
    line.split(";")[1],
    line.split(";")[2],
    line.split(";")[5],
    line.split(";")[8])).collect()))

# Display the Car name, MPG, Cylinders, Weight and Origin for the cars Originating in either Europe or Japan

# Car name is column  0
print((cars_rest.filter(lambda line: line.split(";")[8] in ['Europe','Japan']).
 map(lambda line: (line.split(";")[0],
    line.split(";")[1],
    line.split(";")[2],
    line.split(";")[5],
    line.split(";")[8])).collect()))

# Creating Dataframes

# Create a totally empty dataframe
from pyspark.sql.types import StructType, StructField
sc = spark.sparkContext
#Create empty df
schema = StructType([])
empty = spark.createDataFrame(sc.emptyRDD(), schema)
print(empty.show())

# Create an empty dataframe with header

schema_header = StructType([StructField("name", StringType(), True)])
empty_with_header = spark.createDataFrame(sc.emptyRDD(), schema_header)
print(empty_with_header.show())

# Create a dataframe with header and data
from pyspark.sql import Row
mylist = [
  {"name":'Alice',"age":13},
  {"name":'Jacob',"age":24},
  {"name":'Betty',"age":135},
]
print(spark.createDataFrame(Row(**x) for x in mylist).show())

# You can achieve the same using this - note that we are using spark context here, not a spark session
from pyspark.sql import Row
df = sc.parallelize([
        Row(name='Alice', age=13),
        Row(name='Jacob', age=24),
        Row(name='Betty', age=135)]).toDF()
print(df.show())

from pyspark.sql import Row
mylist = [
  {"name":'Alice',"age":5,"height":80},
  {"name":'Jacob',"age":24,"height":80},
  {"name":'Alice',"age":5,"height":80}
]
df = spark.createDataFrame(Row(**x) for x in mylist)
print(df.dropDuplicates().show())

print(df.dropDuplicates(subset=['height']).show())